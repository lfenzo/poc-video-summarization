{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f7ea65-f7db-4908-8174-1071851c860f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:04.494588Z",
     "iopub.status.busy": "2024-07-12T13:26:04.494491Z",
     "iopub.status.idle": "2024-07-12T13:26:06.711125Z",
     "shell.execute_reply": "2024-07-12T13:26:06.710936Z",
     "shell.execute_reply.started": "2024-07-12T13:26:04.494579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/enzo/.cache/huggingface/hub/models--bartowski--gemma-2-27b-it-GGUF/snapshots/db2e3d6cf286ddaa657e59f52bffcb992dd14358/./gemma-2-27b-it-Q5_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llama_model_loader: - type q5_K:  278 tensors\n",
      "llama_model_loader: - type q6_K:   44 tensors\n",
      "llm_load_vocab: special tokens cache size = 364\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 18.34 GiB (5.78 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.45 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 28/47 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 18775.71 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 10692.21 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    72.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   112.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  1704.31 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 238\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '17', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/gemma-2-27b-it-GGUF\",\n",
    "    filename=\"gemma-2-27b-it-Q5_K_L.gguf\",\n",
    "    verbose=True,\n",
    "    n_gpu_layers=28,\n",
    "    #model_path=model_path,\n",
    "    #n_ctx=2048,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e289976-4e0e-45f6-9b8f-b7542aa35062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:26:36.180167Z",
     "iopub.status.busy": "2024-07-12T13:26:36.180034Z",
     "iopub.status.idle": "2024-07-12T13:26:50.899487Z",
     "shell.execute_reply": "2024-07-12T13:26:50.899232Z",
     "shell.execute_reply.started": "2024-07-12T13:26:36.180159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1952.99 ms\n",
      "llama_print_timings:      sample time =      84.37 ms /    94 runs   (    0.90 ms per token,  1114.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1952.89 ms /     5 tokens (  390.58 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:        eval time =   12598.01 ms /    93 runs   (  135.46 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:       total time =   14715.35 ms /    98 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I am a large language model, and I am trained on a massive amount of text data. This data includes both formal and informal language, including instances of swearing.\n",
      "\n",
      "However, my purpose is to provide helpful and harmless information while adhering to ethical guidelines. As such, I am programmed to avoid generating responses that contain offensive or inappropriate language, including swear words.\n",
      "\n",
      "If you have any other questions or need assistance with a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "response = llm(\n",
    "    \"Can you swear?\",\n",
    "    max_tokens=2048, \n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755f8a4-1639-4c39-969b-fff22e9e5a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T14:31:12.368941Z",
     "iopub.status.busy": "2024-07-11T14:31:12.368803Z",
     "iopub.status.idle": "2024-07-11T14:31:12.372623Z",
     "shell.execute_reply": "2024-07-11T14:31:12.372386Z",
     "shell.execute_reply.started": "2024-07-11T14:31:12.368933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d054394-7782-4d92-86ee-67f0d112a319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:16:10.422771Z",
     "iopub.status.busy": "2024-07-12T13:16:10.422635Z",
     "iopub.status.idle": "2024-07-12T13:16:10.424977Z",
     "shell.execute_reply": "2024-07-12T13:16:10.424794Z",
     "shell.execute_reply.started": "2024-07-12T13:16:10.422759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 5, 'completion_tokens': 34, 'total_tokens': 39}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5d93ce-3515-41a7-9c7f-379f808fdae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:16:08.634749Z",
     "iopub.status.busy": "2024-07-12T13:16:08.634625Z",
     "iopub.status.idle": "2024-07-12T13:16:08.636635Z",
     "shell.execute_reply": "2024-07-12T13:16:08.636427Z",
     "shell.execute_reply.started": "2024-07-12T13:16:08.634742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_completion'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda88deb-4fa8-4026-b333-aa7928153156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:15:55.710055Z",
     "iopub.status.busy": "2024-07-12T13:15:55.709749Z",
     "iopub.status.idle": "2024-07-12T13:15:55.712513Z",
     "shell.execute_reply": "2024-07-12T13:15:55.712287Z",
     "shell.execute_reply.started": "2024-07-12T13:15:55.710045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/enzo/.cache/huggingface/hub/models--bartowski--gemma-2-27b-it-GGUF/snapshots/db2e3d6cf286ddaa657e59f52bffcb992dd14358/./gemma-2-27b-it-IQ2_M.gguf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['model']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
